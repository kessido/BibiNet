{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fn = 'data/bibi_cabinet_meetings_indexed_trimed/0001.mp4'\n",
    "video_frames_per_seconds = 20\n",
    "audio_fn = 'audio_features/wav_files/0001.mp4.wav'\n",
    "output_fn = '0001_res.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVERY_N_SECONDS = 1.0/video_frames_per_seconds\n",
    "temp_video_folder = 'temp_video_folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters (Do Not Change! (unless you change the models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_frames_per_seconds = 200\n",
    "bibi_inpainting_model = './video_model.pt'\n",
    "audio_to_lips_model = './audio_model.h5'\n",
    "face_predictor_path = './shape_predictor_68_face_landmarks.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "from python_speech_features import logfbank, mfcc\n",
    "import cv2\n",
    "import dlib\n",
    "import pickle as pkl\n",
    "from imutils import face_utils\n",
    "import moviepy.editor as mpe\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input,Dense,LSTM,Dropout,LeakyReLU,Activation\n",
    "from keras.layers import TimeDistributed,Flatten,Conv2D,Reshape,concatenate,Lambda\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import scipy.misc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_video_fn, input_audio_fn):\n",
    "    assert('wav' in input_audio_fn)\n",
    "    rate, sig = wav.read(input_audio_fn)\n",
    "    mfccFeat = mfcc(sig, rate)\n",
    "    fbankFeat = logfbank(sig, rate)\n",
    "    audio_features = np.concatenate((mfccFeat, fbankFeat), axis=1)\n",
    "    \n",
    "    detector, predictor = dlib.get_frontal_face_detector(), dlib.shape_predictor(face_predictor_path)\n",
    "    video_path = input_video_fn\n",
    "    video = mpe.VideoFileClip(video_path)\n",
    "    video_duration = video.duration\n",
    "\n",
    "    res, res_cut_params, res_masked, res_ff = [], [], [], []\n",
    "    frame = video.get_frame(0)\n",
    "    frame_width = frame.shape[1]\n",
    "    frame_height = frame.shape[0]\n",
    "\n",
    "    def get_face_features_from_frame(frame,last_frame_rects):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        rects = detector(frame, 0)\n",
    "\n",
    "        if rects is None or len(rects)!=1:\n",
    "            rects = last_frame_rects\n",
    "        if len(rects) != 1:\n",
    "            raise Exception(\"WHOA??! More than one face in the image!\")\n",
    "\n",
    "        dlib_features = predictor(gray, rects[0])\n",
    "        return face_utils.shape_to_np(dlib_features),rects\n",
    "    \n",
    "    last_frame_rects = dlib.rectangles()\n",
    "    last_frame_rects.append(\n",
    "        dlib.rectangle(int(frame_width/4),int(frame_height/4),int(frame_width/4),int(frame_height/4)))\n",
    "    \n",
    "    with tqdm(np.arange(0, video_duration, EVERY_N_SECONDS), desc='processing_frames') as _tqdm:\n",
    "        for frame_time in _tqdm:\n",
    "            frame = video.get_frame(frame_time)\n",
    "            face_features,last_frame_rects = get_face_features_from_frame(frame,last_frame_rects)\n",
    "            frame = np.array(frame, dtype=np.float32)/256\n",
    "            \n",
    "            size_i = face_features[:,0].max() - face_features[:,0].min()\n",
    "            size_j = size_i\n",
    "            mean_j, mean_i = int(np.median(face_features[48:,0])), int(np.median(face_features[48:,1]))\n",
    "            size_i_3 = int(size_i/3)\n",
    "            size_j_2 = int(size_j/2)\n",
    "            frame_cut = [mean_i-3*size_i_3,mean_i+size_i_3,mean_j-size_j_2,mean_j+size_j_2]\n",
    "            frame_masked = frame[frame_cut[0]:frame_cut[1],frame_cut[2]:frame_cut[3]]\n",
    "            frame_masked = resize(frame_masked, (100,74), anti_aliasing=True).astype(np.float32)\n",
    "            frame_masked[67:87, 16:-16] = -1\n",
    "            \n",
    "            face_features = face_features.astype(np.float32)\n",
    "            face_features[:,0] -= face_features[:,0].min()\n",
    "            face_features[:,1] -= face_features[:,1].min()\n",
    "            face_features[:,0] /= face_features[:,0].max()\n",
    "            face_features[:,1] /= face_features[:,1].max()\n",
    "\n",
    "            res.append(np.array(frame))\n",
    "            res_cut_params.append(frame_cut)\n",
    "            res_masked.append(frame_masked)\n",
    "            res_ff.append(face_features)\n",
    "        \n",
    "    res, res_cut_params, res_ff = np.array(res), np.array(res_cut_params), np.array(res_ff)\n",
    "    res_masked = np.array(res_masked)\n",
    "    return res, res_cut_params, res_masked, res_ff, audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 22:34:50.599924 140279326533376 sigproc.py:82] frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "W0710 22:34:52.741321 140279326533376 sigproc.py:82] frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "processing_frames: 100%|██████████| 200/200 [00:34<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "frames, frames_cut_params, frames_masked, frames_face_features, audio_features = load_data(video_fn, audio_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = min(frames.shape[0]/video_frames_per_seconds, \n",
    "               audio_features.shape[0]/audio_frames_per_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 22:35:30.517032 140279326533376 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0710 22:35:30.539246 140279326533376 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0710 22:35:30.554142 140279326533376 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0710 22:35:30.554769 140279326533376 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0710 22:35:30.562920 140279326533376 deprecation.py:506] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0710 22:35:30.822994 140279326533376 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_combined_model():\n",
    "    face_input =Input(shape=(48*2,),          name='face_input',  dtype='float32')\n",
    "    audio_input=Input(shape=(100, 39,), name='audio_input', dtype='float32')\n",
    "    \n",
    "    # face layers:\n",
    "    face=Dense(32, activation='sigmoid',               name='face_dense_1')     (face_input)    \n",
    "    face=Dropout(0.4,                                  name='face_dropout_1')   (face)\n",
    "    face=Dense(5,                                      name='face_dense_2')     (face)\n",
    "    \n",
    "    face_only_output = Dense(40, activation='sigmoid', name='face_only_output') (face)\n",
    "        \n",
    "    # audio layer:\n",
    "    audio = TimeDistributed(\n",
    "        Dense(30, activation='sigmoid',    name='audio_TD_1'))      (audio_input)\n",
    "    audio = Dropout(0.2,                   name='audio_dropout_1') (audio)\n",
    "    audio = LSTM(25,return_sequences=True, name='audio_lstm_1')    (audio)\n",
    "    audio = Reshape((1,100,25),      name='audio_reshape_1') (audio)\n",
    "    audio = Dropout(0.4,                   name='audio_dropout_2') (audio)\n",
    "    audio = Conv2D(20, kernel_size=(25,25), strides=5, activation='sigmoid',\n",
    "       data_format='channels_first',       name='audio_conv2d_1')  (audio)\n",
    "    audio = Flatten(                       name='audio_flatten_1') (audio)\n",
    "    audio = Dropout(0.4,                   name='audio_dropout_3') (audio)\n",
    "    audio = Dense(5, activation='sigmoid', name='audio_dense_1')   (audio)\n",
    "    \n",
    "    audio_boosting = Dense(40, \n",
    "        activation='sigmoid',              name='audio_boosting')   (audio)\n",
    "    audio_boosting = Lambda(lambda x:x*2-1,name='audio_boosting_lambda')(\n",
    "                                                                    audio_boosting)\n",
    "    audio_boosting_output=keras.layers.Add(name='audio_boosting_output')(\n",
    "                                                                    [face_only_output, audio_boosting])\n",
    "        \n",
    "    #concatenate_layer:\n",
    "    conc = concatenate([audio, face],      name='concatenation_1')\n",
    "    conc = Dense(20, activation='sigmoid', name='conc_dense_1') (conc)\n",
    "    \n",
    "    conc_output = Dense(40,\n",
    "        activation='sigmoid',              name='conc_output')  (conc)\n",
    "\n",
    "    model_combined  = Model(inputs=[audio_input, face_input],\n",
    "                            outputs=[face_only_output, audio_boosting_output, conc_output])\n",
    "    return model_combined\n",
    "\n",
    "audio_model = create_combined_model()\n",
    "audio_model.load_weights(audio_to_lips_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadpkl(fn):\n",
    "    with open(fn,'rb') as f:\n",
    "        return pkl.load(f)\n",
    "scalerX=       loadpkl('scalerX')\n",
    "scalerX_extra= loadpkl('scalerX_extra')\n",
    "scalery=       loadpkl('scalery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(duration>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_features_for_prediction():\n",
    "    len_X = int(duration * video_frames_per_seconds)\n",
    "    audio_features_X = []\n",
    "    audio_features_FF = np.array(frames_face_features[:len_X])\n",
    "    for i in range(len_X):\n",
    "        i_mid = int(i * audio_frames_per_seconds / video_frames_per_seconds)\n",
    "        i_start = i_mid - 50\n",
    "        i_end = i_mid + 50\n",
    "        if i_start < 0:\n",
    "            i_end -= i_start\n",
    "            i_start = 0\n",
    "        elif i_end > audio_features.shape[0]:\n",
    "            dif = i_end - audio_features.shape[0]\n",
    "            i_start -= dif\n",
    "            i_end -= dif\n",
    "        audio_features_X.append(audio_features[i_start:i_end])\n",
    "    return np.array(audio_features_X), audio_features_FF[:,:48], audio_features_FF[:, 48:]\n",
    "\n",
    "audio_features_X, audio_features_X_FF, audio_features_y = get_audio_features_for_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features_X = scalerX.transform(audio_features_X.reshape(audio_features_X.shape[0],-1)).reshape(-1,100,39)\n",
    "audio_features_X_FF = scalerX_extra.transform(audio_features_X_FF.reshape((audio_features_X_FF.shape[0],-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lips = audio_model.predict({'audio_input': audio_features_X, 'face_input': audio_features_X_FF})[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lips = scalery.inverse_transform(lips)\n",
    "lips = lips.reshape(-1,20,2)\n",
    "lips[:,:,0] += audio_features_y[:,:,0].mean(axis=1, keepdims=True)\n",
    "lips[:,:,1] += audio_features_y[:,:,1].mean(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features_X_FF = scalerX_extra.inverse_transform(audio_features_X_FF).reshape((-1,48,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_features_predicted = np.concatenate((audio_features_X_FF, lips), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2, inplace=False)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 4, 2, (1,0))\n",
    "        self.conv2 = nn.Conv2d(6, 12, 4, 2, (0,1))\n",
    "        self.batchnorm1 = nn.BatchNorm2d(12)\n",
    "        self.conv3 = nn.Conv2d(12, 12, 4, 2, (1,0))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(12)\n",
    "        self.conv4 = nn.Conv2d(12, 12, 4, 2, 1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(12)\n",
    "        self.linear1 = nn.Linear(288, 30)\n",
    "        self.linear2 = nn.Linear(30, 60)\n",
    "        \n",
    "        self.linear1_ff = nn.Linear(136, 40)\n",
    "        self.linear2_ff = nn.Linear(40, 10)\n",
    "        \n",
    "        self.up_linear1 = nn.Linear(70, 30)\n",
    "        self.up_linear2 = nn.Linear(30, 288)\n",
    "        \n",
    "        self.up_stage_0 = nn.Conv2d(24, 3, 3, 1, 1)\n",
    "\n",
    "        self.up_convt1 = nn.ConvTranspose2d(24,12,4,2,1)\n",
    "        self.up_batchnorm1 = nn.BatchNorm2d(12)\n",
    "        self.up_stage_1 = nn.Conv2d(24, 3, 3, 1, 1)\n",
    "\n",
    "        self.up_convt2 = nn.ConvTranspose2d(24,12,4,2,(1,0))\n",
    "        self.up_batchnorm2 = nn.BatchNorm2d(12)\n",
    "        self.up_stage_2 = nn.Conv2d(24, 3, 3, 1, 1)\n",
    "\n",
    "        self.up_convt3 = nn.ConvTranspose2d(24,6,4,2,(0,1))\n",
    "        self.up_batchnorm3 = nn.BatchNorm2d(6)\n",
    "        self.up_stage_3 = nn.Conv2d(12, 3, 3, 1, 1)\n",
    "\n",
    "        self.up_convt4 = nn.ConvTranspose2d(12,3,4,2,(1,0))\n",
    "        self.up_batchnorm4 = nn.BatchNorm2d(3)\n",
    "        \n",
    "        self.res_conv1 = nn.Conv2d(3,10,5,1,2)\n",
    "        self.res_conv2 = nn.Conv2d(10,3,5,1,2)\n",
    "        self.res_conv3 = nn.Conv2d(6,3,5,1,2)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.upsample = nn.Upsample((100,74))\n",
    "        \n",
    "    def forward(self, original_image, face_features, return_stage=-1):\n",
    "        y = self.leakyReLU(self.linear1_ff(face_features))\n",
    "        y = self.leakyReLU(self.linear2_ff(y))\n",
    "        \n",
    "        x_down_1 =original_image.clone()\n",
    "        x_down_1[original_image==-1]=0\n",
    "        \n",
    "        x_down_2 = self.leakyReLU(self.conv1(x_down_1))\n",
    "\n",
    "        x_down_3 = self.conv2(x_down_2)\n",
    "        x_down_3 = self.leakyReLU(self.batchnorm1(x_down_3))\n",
    "\n",
    "        x_down_4 = self.conv3(x_down_3)\n",
    "        x_down_4 = self.leakyReLU(self.batchnorm2(x_down_4))\n",
    "\n",
    "        x_down_5 = self.conv4(x_down_4)\n",
    "        x_down_5 = self.leakyReLU(self.batchnorm3(x_down_5))\n",
    "\n",
    "        x = x_down_5.reshape((-1,288))\n",
    "        \n",
    "        x = self.leakyReLU(self.linear1(x))\n",
    "        x = self.leakyReLU(self.linear2(x))\n",
    "\n",
    "        x = torch.cat((x,y), dim=1)\n",
    "        \n",
    "        x = self.leakyReLU(self.up_linear1(x))\n",
    "        x = self.leakyReLU(self.up_linear2(x))\n",
    "        \n",
    "        x = x.reshape((-1, 12, 6, 4))\n",
    "        \n",
    "        x = torch.cat((x,x_down_5), dim=1)\n",
    "        x0 = self.upsample(self.sigmoid(self.up_stage_0(x)))\n",
    "        \n",
    "        x = self.leakyReLU(self.up_batchnorm1(self.up_convt1(x)))\n",
    "        x = torch.cat((x,x_down_4), dim=1)\n",
    "        x1 = self.upsample(self.sigmoid(self.up_stage_1(x)))\n",
    "        \n",
    "        x = self.leakyReLU(self.up_batchnorm2(self.up_convt2(x)))\n",
    "        x = torch.cat((x,x_down_3), dim=1)\n",
    "        x2 = self.upsample(self.sigmoid(self.up_stage_2(x)))\n",
    "        \n",
    "        x = self.leakyReLU(self.up_batchnorm3(self.up_convt3(x)))\n",
    "        x = torch.cat((x,x_down_2), dim=1)\n",
    "        x3 = self.upsample(self.sigmoid(self.up_stage_3(x)))\n",
    "        \n",
    "        x = self.sigmoid(self.up_batchnorm4(self.up_convt4(x)))\n",
    "        x4 = x\n",
    "        \n",
    "        x = x.clone()\n",
    "        x[original_image!=-1] = original_image[original_image!=-1]\n",
    "        \n",
    "        x_start = x.clone()\n",
    "        x = self.leakyReLU(self.res_conv1(x))\n",
    "        x = self.leakyReLU(self.res_conv2(x))\n",
    "        x = torch.cat((x,x_start),dim=1)\n",
    "        x = torch.sigmoid(self.res_conv3(x)).clone()\n",
    "        mask = original_image!=-1\n",
    "        x[mask] = original_image[mask]\n",
    "        x5 = x\n",
    "        \n",
    "        return x0, x1, x2, x3, x4, x5\n",
    "\n",
    "generator = Generator()\n",
    "generator = generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (leakyReLU): LeakyReLU(negative_slope=0.2)\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(4, 4), stride=(2, 2), padding=(1, 0))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(4, 4), stride=(2, 2), padding=(0, 1))\n",
       "  (batchnorm1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(12, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 0))\n",
       "  (batchnorm2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(12, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (batchnorm3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=288, out_features=30, bias=True)\n",
       "  (linear2): Linear(in_features=30, out_features=60, bias=True)\n",
       "  (linear1_ff): Linear(in_features=136, out_features=40, bias=True)\n",
       "  (linear2_ff): Linear(in_features=40, out_features=10, bias=True)\n",
       "  (up_linear1): Linear(in_features=70, out_features=30, bias=True)\n",
       "  (up_linear2): Linear(in_features=30, out_features=288, bias=True)\n",
       "  (up_stage_0): Conv2d(24, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (up_convt1): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (up_batchnorm1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (up_stage_1): Conv2d(24, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (up_convt2): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 0))\n",
       "  (up_batchnorm2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (up_stage_2): Conv2d(24, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (up_convt3): ConvTranspose2d(24, 6, kernel_size=(4, 4), stride=(2, 2), padding=(0, 1))\n",
       "  (up_batchnorm3): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (up_stage_3): Conv2d(12, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (up_convt4): ConvTranspose2d(12, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 0))\n",
       "  (up_batchnorm4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (res_conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (res_conv2): Conv2d(10, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (res_conv3): Conv2d(6, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (sigmoid): Sigmoid()\n",
       "  (upsample): Upsample(size=(100, 74), mode=nearest)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.load_state_dict(torch.load(bibi_inpainting_model))\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predicting video: 100%|██████████| 200/200 [00:02<00:00, 73.11it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(range(face_features_predicted.shape[0]),desc='predicting video') as _tqdm:\n",
    "    for i in _tqdm:\n",
    "        pred = generator(\n",
    "            torch.Tensor([frames_masked[i].transpose([2,0,1])]).to(device),\n",
    "            torch.Tensor([face_features_predicted[i].reshape(-1)]).to(device)\n",
    "        )[5].detach().cpu().numpy().transpose([0,2,3,1])[0]\n",
    "        i1,i2,j1,j2 = frames_cut_params[i]\n",
    "        i_size,j_size = i2-i1, j2-j1\n",
    "        frames[i,i1:i2,j1:j2]=resize(pred,(i_size,j_size),anti_aliasing=True).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(temp_video_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving temp images:   0%|          | 0/200 [00:00<?, ?it/s]W0710 22:39:16.063871 140279326533376 utils.py:100] /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "\n",
      "saving temp images: 100%|██████████| 200/200 [00:28<00:00,  6.75it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(range(frames_masked.shape[0]), desc='saving temp images') as _tqdm:\n",
    "    for i in _tqdm:\n",
    "        scipy.misc.toimage(frames[i], cmin=0.0, cmax=1.0).save(os.path.join(temp_video_folder,'%05d.png' % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.2.12-1~deb9u1 Copyright (c) 2000-2018 the FFmpeg developers\n",
      "  built with gcc 6.3.0 (Debian 6.3.0-18+deb9u1) 20170516\n",
      "  configuration: --prefix=/usr --extra-version='1~deb9u1' --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libebur128 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 34.101 / 55. 34.101\n",
      "  libavcodec     57. 64.101 / 57. 64.101\n",
      "  libavformat    57. 56.101 / 57. 56.101\n",
      "  libavdevice    57.  1.100 / 57.  1.100\n",
      "  libavfilter     6. 65.100 /  6. 65.100\n",
      "  libavresample   3.  1.  0 /  3.  1.  0\n",
      "  libswscale      4.  2.100 /  4.  2.100\n",
      "  libswresample   2.  3.100 /  2.  3.100\n",
      "  libpostproc    54.  1.100 / 54.  1.100\n",
      "Input #0, image2, from 'temp_video_folder/%05d.png':\n",
      "  Duration: 00:00:10.00, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 1280x720, 20 fps, 20 tbr, 20 tbn, 20 tbc\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #1.0 : stereo\n",
      "\u001b[0mInput #1, wav, from 'audio_features/wav_files/0001.mp4.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.56.101\n",
      "  Duration: 00:02:35.99, bitrate: 1411 kb/s\n",
      "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "\u001b[0;33mNo pixel format specified, yuv444p for H.264 encoding chosen.\n",
      "Use -pix_fmt yuv420p for compatibility with outdated media players.\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 AVX2 LZCNT BMI2\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mprofile High 4:4:4 Predictive, level 3.1, 4:4:4 8-bit\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0m264 - core 148 r2748 97eaef2 - H.264/MPEG-4 AVC codec - Copyleft 2003-2016 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=20 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'temp_video_.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.56.101\n",
      "    Stream #0:0: Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv444p, 1280x720, q=-1--1, 20 fps, 10240 tbn, 20 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc57.64.101 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "    Stream #0:1: Audio: aac (LC) ([64][0][0][0] / 0x0040), 44100 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc57.64.101 aac\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[0;35m[image2 @ 0x5627308c7a20] \u001b[0m\u001b[0;33mThread message queue blocking; consider raising the thread_queue_size option (current value: 8)\n",
      "frame=  200 fps= 54 q=-1.0 Lsize=    1300kB time=00:00:10.00 bitrate=1064.0kbits/s speed= 2.7x    \n",
      "video:1136kB audio:156kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.589684%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mframe I:1     Avg QP:17.91  size: 31509\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mframe P:51    Avg QP:19.18  size: 12827\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mframe B:148   Avg QP:22.40  size:  3222\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mconsecutive B-frames:  1.0%  1.0%  0.0% 98.0%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mmb I  I16..4: 25.4% 61.6% 12.9%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mmb P  I16..4:  6.8% 12.6%  0.6%  P16..4: 31.3% 12.1%  4.0%  0.0%  0.0%    skip:32.7%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mmb B  I16..4:  0.7%  0.9%  0.0%  B16..8: 35.8%  2.9%  0.2%  direct: 0.5%  skip:59.0%  L0:53.7% L1:43.5% BI: 2.9%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0m8x8 transform intra:61.7% inter:91.6%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mcoded y,u,v intra: 29.0% 12.2% 13.4% inter: 6.2% 2.3% 3.3%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mi16 v,h,dc,p: 43% 28% 18% 11%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 17% 37%  3%  3%  4%  3%  3%  3%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30% 38% 12%  3%  3%  4%  4%  4%  3%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mref P L0: 57.9%  8.2% 21.4% 12.5%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mref B L0: 88.5%  9.2%  2.4%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mref B L1: 95.1%  4.9%\n",
      "\u001b[1;36m[libx264 @ 0x562730900bc0] \u001b[0mkb/s:930.05\n",
      "\u001b[1;36m[aac @ 0x562730911900] \u001b[0mQavg: 673.458\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR_FMT = os.path.join(temp_video_folder,'%05d.png')\n",
    "!ffmpeg -framerate $video_frames_per_seconds -i $IMAGE_DIR_FMT -i $audio_fn -shortest temp_video_.mp4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.2.12-1~deb9u1 Copyright (c) 2000-2018 the FFmpeg developers\n",
      "  built with gcc 6.3.0 (Debian 6.3.0-18+deb9u1) 20170516\n",
      "  configuration: --prefix=/usr --extra-version='1~deb9u1' --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libebur128 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 34.101 / 55. 34.101\n",
      "  libavcodec     57. 64.101 / 57. 64.101\n",
      "  libavformat    57. 56.101 / 57. 56.101\n",
      "  libavdevice    57.  1.100 / 57.  1.100\n",
      "  libavfilter     6. 65.100 /  6. 65.100\n",
      "  libavresample   3.  1.  0 /  3.  1.  0\n",
      "  libswscale      4.  2.100 /  4.  2.100\n",
      "  libswresample   2.  3.100 /  2.  3.100\n",
      "  libpostproc    54.  1.100 / 54.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'temp_video_.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf57.56.101\n",
      "  Duration: 00:00:10.03, start: 0.000000, bitrate: 1061 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x720, 930 kb/s, 20 fps, 20 tbr, 10240 tbn, 40 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #1.0 : stereo\n",
      "\u001b[0mInput #1, wav, from 'audio_features/wav_files/0001.mp4.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.56.101\n",
      "  Duration: 00:02:35.99, bitrate: 1411 kb/s\n",
      "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "\u001b[0;33mNo pixel format specified, yuv444p for H.264 encoding chosen.\n",
      "Use -pix_fmt yuv420p for compatibility with outdated media players.\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 AVX2 LZCNT BMI2\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mprofile High 4:4:4 Predictive, level 3.1, 4:4:4 8-bit\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0m264 - core 148 r2748 97eaef2 - H.264/MPEG-4 AVC codec - Copyleft 2003-2016 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=20 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '0001_res.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf57.56.101\n",
      "    Stream #0:0(und): Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv444p, 1280x720, q=-1--1, 20 fps, 10240 tbn, 20 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc57.64.101 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "    Stream #0:1(und): Audio: aac (LC) ([64][0][0][0] / 0x0040), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      encoder         : Lavc57.64.101 aac\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "  Stream #0:1 -> #0:1 (aac (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "frame=  200 fps= 64 q=-1.0 Lsize=    1200kB time=00:00:10.00 bitrate= 982.2kbits/s speed= 3.2x    \n",
      "video:1035kB audio:157kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.639142%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mframe I:1     Avg QP:17.84  size: 30288\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mframe P:53    Avg QP:18.91  size: 11600\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mframe B:146   Avg QP:22.55  size:  2835\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mconsecutive B-frames:  1.0%  5.0%  0.0% 94.0%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mmb I  I16..4: 22.9% 65.6% 11.5%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mmb P  I16..4:  5.6% 11.1%  0.4%  P16..4: 32.6% 11.1%  3.7%  0.0%  0.0%    skip:35.6%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mmb B  I16..4:  0.5%  0.7%  0.0%  B16..8: 33.2%  2.4%  0.2%  direct: 0.3%  skip:62.8%  L0:51.6% L1:45.5% BI: 2.9%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0m8x8 transform intra:64.1% inter:93.2%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mcoded y,u,v intra: 30.0% 12.7% 13.9% inter: 5.4% 2.1% 2.9%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mi16 v,h,dc,p: 41% 28% 19% 12%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 17% 36%  3%  3%  4%  3%  3%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 38% 12%  3%  4%  5%  4%  4%  2%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mref P L0: 58.3%  8.6% 22.1% 11.0%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mref B L0: 86.8% 10.6%  2.6%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mref B L1: 95.9%  4.1%\n",
      "\u001b[1;36m[libx264 @ 0x55be6fc9d240] \u001b[0mkb/s:847.23\n",
      "\u001b[1;36m[aac @ 0x55be6fc6ecc0] \u001b[0mQavg: 668.686\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i temp_video_.mp4 -i $audio_fn -shortest $output_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.2.12-1~deb9u1 Copyright (c) 2000-2018 the FFmpeg developers\r\n",
      "  built with gcc 6.3.0 (Debian 6.3.0-18+deb9u1) 20170516\r\n",
      "  configuration: --prefix=/usr --extra-version='1~deb9u1' --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libebur128 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\r\n",
      "  libavutil      55. 34.101 / 55. 34.101\r\n",
      "  libavcodec     57. 64.101 / 57. 64.101\r\n",
      "  libavformat    57. 56.101 / 57. 56.101\r\n",
      "  libavdevice    57.  1.100 / 57.  1.100\r\n",
      "  libavfilter     6. 65.100 /  6. 65.100\r\n",
      "  libavresample   3.  1.  0 /  3.  1.  0\r\n",
      "  libswscale      4.  2.100 /  4.  2.100\r\n",
      "  libswresample   2.  3.100 /  2.  3.100\r\n",
      "  libpostproc    54.  1.100 / 54.  1.100\r\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '0001_res.mp4':\r\n",
      "  Metadata:\r\n",
      "    major_brand     : isom\r\n",
      "    minor_version   : 512\r\n",
      "    compatible_brands: isomiso2avc1mp41\r\n",
      "    encoder         : Lavf57.56.101\r\n",
      "  Duration: 00:00:10.03, start: 0.000000, bitrate: 979 kb/s\r\n",
      "    Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1280x720, 847 kb/s, 20 fps, 20 tbr, 10240 tbn, 40 tbc (default)\r\n",
      "    Metadata:\r\n",
      "      handler_name    : VideoHandler\r\n",
      "    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\r\n",
      "    Metadata:\r\n",
      "      handler_name    : SoundHandler\r\n",
      "\u001b[4;31mAt least one output file must be specified\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i $output_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('temp_video_.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_video_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
